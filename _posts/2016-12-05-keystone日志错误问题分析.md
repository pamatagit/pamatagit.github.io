---
layout: post
title: keystone日志错误问题分析
date: 2016-12-05
category: "keystone"
---

## 问题描述

在日志体检时，发现楚天云R1的controller1节点，keystone-manage.log中有如下错误信息：

> root@r1201001controller:~# cat /var/log/keystone/keystone-manage.log |grep ERROR
> ...
> 2016-06-30 01:30:43.558 34159 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-01 01:30:38.483 14888 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-02 01:30:27.579 58757 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-06 01:30:34.269 28271 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-16 01:31:06.171 21230 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-18 01:30:50.827 31491 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-20 01:30:34.802 51664 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-23 01:30:47.050 46276 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-24 01:30:51.544 18920 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> ...

## 问题分析

查看keystone-manage.log，发现有以下信息：

> 2016-07-10 01:30:03.019 36672 TRACE keystone DBDeadlock: (OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') 'DELETE FROM token WHERE token.expires <= %s' (datetime.datetime(2016, 6, 23, 4, 18, 41),)

该信息表示在删除token时发生了deadlock。

另外，楚天云中有一个定期任务，每天1:30会执行`keystone-manage token_flush`删除过期的token，时间上与错误日志相符。

根据以上信息，可以知道该错误是由每天定时执行的`keystone-manage token_flush`引起的。

而该定期任务在2个controller节点上都有运行，查看controller2节点的日志，也发现类似信息：

> root@r1201002controller:~# cat /var/log/keystone/keystone-manage.log |grep ERROR
> ...
> 2016-07-03 01:30:02.749 64259 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-04 01:31:02.030 33293 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-05 01:30:31.604 64691 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-11 01:30:48.575 60880 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-12 01:30:16.973 34709 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-15 01:31:52.946 18657 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-17 01:31:11.538 42609 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-19 01:30:58.180 58905 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-22 01:30:02.491 3101 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-27 01:30:29.680 7338 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> 2016-07-30 01:30:27.150 53294 ERROR oslo_db.sqlalchemy.exc_filters [-] DB exception wrapped.
> ...

比较controller1和controller2的日志，发现当controller1报错的时候，controller2没有报错；controller2报错的时候，controller1没有报错。说明2台节点同时执行`token_flush`时，有一台节点执行成功，而另一台节点执行失败，并报错。

## 问题原因

根据上面的分析，可以知道该错误是由于2台controller节点同时执行`keystone-manage token_flush`删除过期token时，发生了数据库deadlock。

## 问题复现

在2台controller节点同时执行`keystone-manage token_flush`，且需要删除的数据量较大时，可以复现该问题。

## 问题影响

进到数据库中查看发现token数量并不多，而且都是当天产生的token，说明token_flush被成功执行了，过期的token被成功删除了。也就是说，该错误并没有对云平台产生实际的影响。

## 解决措施

1.可以关闭1台controller节点上的定时任务，保证只有1台controller节点会执行token flush操作。

2.由于该错误对云平台业务没有影响，可以忽略该问题。